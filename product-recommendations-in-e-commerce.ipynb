{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAuNjO7d8DXA",
        "outputId": "e2779f55-0410-40c5-8058-62ec912892e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise (from surprise)\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.11.4)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-linux_x86_64.whl size=3162988 sha256=12742d8307a5b1117f992eed942ca03fa5dd519003711eb601896c2eff47977a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/ca/a8/4e28def53797fdc4363ca4af740db15a9c2f1595ebc51fb445\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.3 surprise-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install surprise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning and data processing\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from surprise import SVD, Dataset, Reader\n",
        "\n",
        "# Logging for debugging\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ],
      "metadata": {
        "id": "6KjGhUTZ8P0r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and preprocess data\n",
        "def load_data(user_data_file, product_data_file):\n",
        "    try:\n",
        "        user_data = pd.read_csv(user_data_file, delimiter='\\t')\n",
        "        product_data = pd.read_csv(product_data_file, delimiter='\\t')\n",
        "\n",
        "        # Handle missing values and scale weights\n",
        "        user_data['rating'] = user_data['weight'] * 5 / user_data['weight'].max()\n",
        "        product_data.fillna(method='ffill', inplace=True)  # Forward filling missing values\n",
        "        logging.info(\"Data loaded and preprocessed successfully.\")\n",
        "        return user_data, product_data\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data: {e}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "7jW1Qs_S8jOK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collaborative filtering module\n",
        "def collaborative_filtering(user_data):\n",
        "    try:\n",
        "        reader = Reader(rating_scale=(1, 5))\n",
        "        data = Dataset.load_from_df(user_data[['userID', 'artistID', 'rating']], reader)\n",
        "        trainset = data.build_full_trainset()\n",
        "        algo = SVD()\n",
        "        algo.fit(trainset)\n",
        "        logging.info(\"Collaborative filtering model trained successfully.\")\n",
        "        return algo\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in collaborative filtering: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "XzGFmZA88tPF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Content-based filtering module with deep learning\n",
        "def content_based_filtering(product_data):\n",
        "    try:\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(product_data['name'])\n",
        "        sequences = tokenizer.texts_to_sequences(product_data['name'])\n",
        "        max_length = max([len(seq) for seq in sequences])\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "        # Deep learning model for product representation\n",
        "        embedding_dim = 128\n",
        "        vocab_size = len(tokenizer.word_index) + 1\n",
        "        model = Sequential([\n",
        "            Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "            LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(product_data.shape[0], activation='sigmoid')\n",
        "        ])\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "        model.fit(padded_sequences, np.eye(product_data.shape[0]), epochs=5, batch_size=32, validation_split=0.1)\n",
        "        product_representations = model.predict(padded_sequences)\n",
        "        product_similarity_matrix = cosine_similarity(product_representations)\n",
        "        logging.info(\"Content-based filtering model trained and similarity matrix created.\")\n",
        "        return product_similarity_matrix\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in content-based filtering: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "OXptVlYV8vqw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization Functions\n",
        "def plot_histogram(recommendations, column='rating'):\n",
        "    if recommendations is not None and column in recommendations.columns:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.hist(recommendations[column], bins=20, color='blue', alpha=0.7)\n",
        "        plt.title('Histogram of Ratings in Recommended Products')\n",
        "        plt.xlabel('Rating')\n",
        "        plt.ylabel('Number of Products')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No valid recommendations to display or specified column missing.\")"
      ],
      "metadata": {
        "id": "fYOi0tE08y1K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_similarity_matrix(matrix, labels=None):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(matrix, xticklabels=labels, yticklabels=labels, cmap='viridis')\n",
        "    plt.title('Product Similarity Matrix')\n",
        "    plt.xlabel('Products')\n",
        "    plt.ylabel('Products')\n",
        "    plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "3VRYd4wN82yV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hybrid recommendation system\n",
        "def hybrid_recommendation(user_id, user_data, product_data, collab_model, content_model, alpha=0.5):\n",
        "    try:\n",
        "        user_interactions = user_data[user_data['userID'] == user_id]\n",
        "        collab_predictions = [collab_model.predict(user_id, product_id).est for product_id in product_data['id']]\n",
        "\n",
        "        user_liked_products = user_interactions[user_interactions['rating'] > 3]['artistID']\n",
        "        user_liked_products_indices = [product_data[product_data['id'] == product_id].index[0] for product_id in user_liked_products]\n",
        "\n",
        "        if user_liked_products_indices:\n",
        "            content_predictions = content_model[user_liked_products_indices].sum(axis=0) / len(user_liked_products_indices)\n",
        "        else:\n",
        "            content_predictions = np.zeros_like(collab_predictions)  # Default to zero or some other logic\n",
        "\n",
        "        hybrid_predictions = alpha * np.array(collab_predictions) + (1 - alpha) * np.array(content_predictions)\n",
        "        recommendations = product_data.iloc[np.argsort(hybrid_predictions)[::-1]]\n",
        "\n",
        "        return recommendations\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in hybrid recommendation: {e}\")\n",
        "        return None\n",
        ""
      ],
      "metadata": {
        "id": "uNFhWPlW85IV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main function to run the recommendation system\n",
        "def main():\n",
        "    user_data, product_data = load_data('user_artists.dat', 'artists.dat')\n",
        "    if user_data is not None and product_data is not None:\n",
        "        collab_model = collaborative_filtering(user_data)\n",
        "        content_model = content_based_filtering(product_data)\n",
        "        if collab_model is not None and content_model is not None:\n",
        "            user_id = 123\n",
        "            recommendations = hybrid_recommendation(user_id, user_data, product_data, collab_model, content_model)\n",
        "            if recommendations is not None and 'rating' in recommendations.columns and not recommendations.empty:\n",
        "                print(recommendations)\n",
        "                plot_histogram(recommendations, 'rating')\n",
        "                plot_similarity_matrix(content_model)\n",
        "            else:\n",
        "                logging.info(\"No valid recommendations to display or 'rating' column missing.\")\n",
        "        else:\n",
        "            logging.error(\"Failed to train models or models returned None.\")\n",
        "    else:\n",
        "        logging.error(\"Data loading failed, cannot proceed with model training.\")"
      ],
      "metadata": {
        "id": "bIYf0_F587sr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niXEFnYP8-fJ",
        "outputId": "1f1a4ca6-5c42-41a0-f5c4-c37d16aed807"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "496/496 [==============================] - 40s 67ms/step - loss: 0.0341 - val_loss: 6.5299e-04\n",
            "Epoch 2/5\n",
            "496/496 [==============================] - 30s 60ms/step - loss: 6.7859e-04 - val_loss: 6.7937e-04\n",
            "Epoch 3/5\n",
            "496/496 [==============================] - 30s 61ms/step - loss: 6.7624e-04 - val_loss: 7.0105e-04\n",
            "Epoch 4/5\n",
            "496/496 [==============================] - 30s 60ms/step - loss: 6.7634e-04 - val_loss: 7.1907e-04\n",
            "Epoch 5/5\n",
            "496/496 [==============================] - 30s 61ms/step - loss: 6.8029e-04 - val_loss: 7.2684e-04\n",
            "551/551 [==============================] - 4s 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from surprise import SVD, Dataset, Reader\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_data(user_data_file, product_data_file):\n",
        "    user_data = pd.read_csv(user_data_file, delimiter='\\t')\n",
        "    product_data = pd.read_csv(product_data_file, delimiter='\\t')\n",
        "\n",
        "    # Preprocess user data (e.g., handle missing values, encode categorical features)\n",
        "    user_data['rating'] = user_data['weight']*5/max(user_data['weight'])\n",
        "\n",
        "    # Preprocess product data (e.g., handle missing values, preprocess text data)\n",
        "    #product_data['rating'] = product_data['weight']*5/max(pruduct_data['weight'])\n",
        "\n",
        "    return user_data, product_data\n",
        "\n",
        "# Collaborative filtering module\n",
        "def collaborative_filtering(user_data):\n",
        "    reader = Reader(rating_scale=(1, 5))\n",
        "    data = Dataset.load_from_df(user_data[['userID', 'artistID', 'rating']], reader)\n",
        "    trainset = data.build_full_trainset()\n",
        "    algo = SVD()\n",
        "    algo.fit(trainset)\n",
        "\n",
        "    return algo\n",
        "\n",
        "\n",
        "# Content-based filtering module with deep learning\n",
        "def content_based_filtering(product_data):\n",
        "    # Text preprocessing\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(product_data['name'])\n",
        "    sequences = tokenizer.texts_to_sequences(product_data['name'])\n",
        "    max_length = max([len(seq) for seq in sequences])\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Deep learning model for product representation\n",
        "    embedding_dim = 128\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(product_data.shape[0], activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    model.fit(padded_sequences, np.eye(product_data.shape[0]), epochs=10, batch_size=32)\n",
        "\n",
        "    # Compute product similarity matrix\n",
        "    product_representations = model.predict(padded_sequences)\n",
        "    product_similarity_matrix = cosine_similarity(product_representations)\n",
        "\n",
        "    return product_similarity_matrix\n",
        "\n",
        "# Hybrid recommendation system\n",
        "def hybrid_recommendation(user_id, user_data, product_data, collab_model, content_model, alpha=0.5):\n",
        "    user_interactions = user_data[user_data['userID'] == user_id]\n",
        "\n",
        "    # Collaborative filtering predictions\n",
        "    collab_predictions = [collab_model.predict(user_id, product_id)[3] for product_id in product_data['id']]\n",
        "\n",
        "    # Content-based filtering predictions\n",
        "    user_liked_products = user_interactions[user_interactions['rating'] > 3]['artistID']\n",
        "    user_liked_products_indices = [product_data[product_data['id'] == product_id].index[0] for product_id in user_liked_products]\n",
        "    content_predictions = content_model[user_liked_products_indices].sum(axis=0) / len(user_liked_products_indices)\n",
        "\n",
        "    # Hybrid recommendations\n",
        "    hybrid_predictions = alpha * np.array(collab_predictions) + (1 - alpha) * np.array(content_predictions)\n",
        "    hybrid_recommendations = product_data.iloc[hybrid_predictions.argsort()[::-1]]\n",
        "\n",
        "    return hybrid_recommendations\n",
        "\n",
        "# Example usage\n",
        "user_data, product_data = load_data('user_artists.dat', 'artists.dat')\n",
        "collab_model = collaborative_filtering(user_data)\n",
        "content_model = content_based_filtering(product_data)\n",
        "\n",
        "user_id = 123  # Example user ID\n",
        "hybrid_recommendations = hybrid_recommendation(user_id, user_data, product_data, collab_model, content_model)\n",
        "print(hybrid_recommendations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2JWpNNe9AsX",
        "outputId": "e6186970-ac4b-4457-ddeb-d0522a909b7c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "551/551 [==============================] - 27s 41ms/step - loss: 0.0285\n",
            "Epoch 2/10\n",
            "551/551 [==============================] - 23s 42ms/step - loss: 6.8287e-04\n",
            "Epoch 3/10\n",
            "551/551 [==============================] - 19s 35ms/step - loss: 6.8630e-04\n",
            "Epoch 4/10\n",
            "551/551 [==============================] - 19s 35ms/step - loss: 6.8811e-04\n",
            "Epoch 5/10\n",
            "551/551 [==============================] - 19s 34ms/step - loss: 6.9034e-04\n",
            "Epoch 6/10\n",
            "551/551 [==============================] - 20s 35ms/step - loss: 6.9179e-04\n",
            "Epoch 7/10\n",
            "551/551 [==============================] - 19s 34ms/step - loss: 6.9194e-04\n",
            "Epoch 8/10\n",
            "551/551 [==============================] - 19s 35ms/step - loss: 6.9009e-04\n",
            "Epoch 9/10\n",
            "551/551 [==============================] - 19s 35ms/step - loss: 6.8767e-04\n",
            "Epoch 10/10\n",
            "551/551 [==============================] - 20s 35ms/step - loss: 6.8458e-04\n",
            "551/551 [==============================] - 4s 7ms/step\n",
            "          id                               name  \\\n",
            "5873    5998    Madonna feat. Justin Timberlake   \n",
            "5874    5999                             Zombie   \n",
            "5875    6000                 LanzamientosMp3.es   \n",
            "5876    6001                             Shamur   \n",
            "5877    6002                      Juliana Pasha   \n",
            "...      ...                                ...   \n",
            "11757  12183   Thao with The Get Down Stay Down   \n",
            "11758  12184                           トクマルシューゴ   \n",
            "11759  12185                           FREENOTE   \n",
            "11760  12186  Ella Fitzgerald & Louis Armstrong   \n",
            "11752  12178                    BEAT!BEAT!BEAT!   \n",
            "\n",
            "                                                     url  \\\n",
            "5873   http://www.last.fm/music/Madonna+feat.+Justin+...   \n",
            "5874                     http://www.last.fm/music/Zombie   \n",
            "5875         http://www.last.fm/music/LanzamientosMp3.es   \n",
            "5876                     http://www.last.fm/music/Shamur   \n",
            "5877              http://www.last.fm/music/Juliana+Pasha   \n",
            "...                                                  ...   \n",
            "11757  http://www.last.fm/music/Thao+with+The+Get+Dow...   \n",
            "11758  http://www.last.fm/music/%E3%83%88%E3%82%AF%E3...   \n",
            "11759                  http://www.last.fm/music/FREENOTE   \n",
            "11760  http://www.last.fm/music/Ella%2BFitzgerald%2B%...   \n",
            "11752     http://www.last.fm/music/BEAT%21BEAT%21BEAT%21   \n",
            "\n",
            "                                              pictureURL  \n",
            "5873   http://userserve-ak.last.fm/serve/252/30705889...  \n",
            "5874    http://userserve-ak.last.fm/serve/252/297390.jpg  \n",
            "5875   http://userserve-ak.last.fm/serve/252/24280293...  \n",
            "5876    http://userserve-ak.last.fm/serve/252/272812.jpg  \n",
            "5877   http://userserve-ak.last.fm/serve/252/47356893...  \n",
            "...                                                  ...  \n",
            "11757  http://userserve-ak.last.fm/serve/252/43814189...  \n",
            "11758  http://userserve-ak.last.fm/serve/252/12556183...  \n",
            "11759   http://userserve-ak.last.fm/serve/252/581912.png  \n",
            "11760    http://userserve-ak.last.fm/serve/252/20171.jpg  \n",
            "11752  http://userserve-ak.last.fm/serve/252/51885229...  \n",
            "\n",
            "[17632 rows x 4 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-3c486bb385e4>:74: RuntimeWarning: invalid value encountered in divide\n",
            "  content_predictions = content_model[user_liked_products_indices].sum(axis=0) / len(user_liked_products_indices)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YMFRkWNf-RZk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}